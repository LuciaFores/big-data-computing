{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ezpoc-_wf5YJ",
        "8LpSWfrcgGHJ",
        "TlHjqPOmreLb"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Needed Library Import\n",
        "In this section the needed library to work with will be imported and some important variables will be initializatied"
      ],
      "metadata": {
        "id": "rNc25BxTSwtQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZhWcP-oSuJm"
      },
      "outputs": [],
      "source": [
        "# In order to make the analysis reproducible\n",
        "RANDOM_SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDU1pSdXS3ok",
        "outputId": "a1263947-9299-4fd5-8229-83401d29a72a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/681.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/681.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m675.8/681.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19867 sha256=8f9d002f1aa094d3377bfe89bbc56f27c003ac0ca61688b57cc7bb67d6f8c2b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/42/78/0c3d438d7f5730451a25f7ac6cbf4391759d22a67576ed7c2c\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2Tn9uykXlYRjLcSLIgmv1iLdEvG_3gqA9M3V4hLGqH9N3JWEr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZw6mvj8TnnK",
        "outputId": "a6a3dc5b-625f-4b1c-d8b5-2fa098d2a7cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "Br58IkI_TrlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark Initialization\n",
        "In this section the connection with PySpark will be initializated"
      ],
      "metadata": {
        "id": "ezpoc-_wf5YJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxrcGP2xTyZl",
        "outputId": "203f277e-9c89-4723-ad39-be5e776e8f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285388 sha256=c730062f802637c706c0d342ce4029e755025d11ebc11049aed445c7cc636112\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as sqlf\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS, ALSModel\n",
        "from pyspark.mllib.evaluation import RankingMetrics"
      ],
      "metadata": {
        "id": "06Q4ABRZS3Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '4G').\\\n",
        "                set('spark.driver.memory', '45G').\\\n",
        "                set('spark.driver.maxResultSize', '10G').\\\n",
        "                setAppName(\"ALSAnalysis\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "pqjSANL6T6sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "port = '4050'\n",
        "public_url = ngrok.connect(port).public_url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJSFff4RT9by",
        "outputId": "a782b453-52f4-4d8b-af5d-36a0a03a974e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-08-19T13:51:52+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"To access the Spark Web UI console, please click on the following link to the ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kK75_JyT_n9",
        "outputId": "60d7f784-a6f6-4484-b2ec-5a1dbf674ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To access the Spark Web UI console, please click on the following link to the ngrok tunnel \"https://5215-35-194-85-157.ngrok-free.app\" -> \"http://127.0.0.1:4050\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "NuGJtC31UAQG",
        "outputId": "78e047ee-ca61-43ca-ff34-cb38bdcc2e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7aa4a0971f00>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b9633fc9dfda:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>ALSAnalysis</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connection to Google Drive"
      ],
      "metadata": {
        "id": "8LpSWfrcgGHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect this colab to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PCbZ10zUFGE",
        "outputId": "7e90e7b7-8c73-494d-a17f-5c38c3f79c29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "TlHjqPOmreLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function is used to compute the Recommender System evaluation metrics.\n",
        "\n",
        "Each metric is computed for each eantry if the dataset (since the function will later on be applied to every row of the dataframe).\n",
        "\n",
        "Each metric is implemented by only giving the implementation of the math formulation of the evaluation metric: specifically the function takes in input two values (that are lists in the dataset):\n",
        "\n",
        "\n",
        "*   **predicted** which is the list of the values predicted for the entry of the database (meaning the items filtered, the one that were not the one used for the training)\n",
        "*   **actual** which is the list of the values that corresponds to the ground truth for the entry of the dataset\n",
        "\n",
        "The implementation of the formulas is the following:\n",
        "\n",
        "\n",
        "*   **Precision@K** and **MAP**: the Precision@K (and consequently also the MAP) checks if the Recommender System is able to predict first the most important (most relevant) items for the user. The formula at the base of this two metrics is the same, the only difference resides in the fact that MAP is computed by computing Precision@K with differente cut-off points (meaning different Ks)\n",
        "\n",
        "  $Precision@K$ = $\\frac{relevant\\_items\\_in\\_the\\_first\\_k\\_positions}{k}$\n",
        "\n",
        "  Where the relevant items are the one that the user interacted in a positive way with\n",
        "\n",
        "\n",
        "*   **Recall@K**: the Recall@K checks if the Recommender System is able to provide sooner or later the relevant recommendations for the user (meaning that **regardless the order** is able to actually recommend the items with the user has interacted in a positive way. The formula of the metric is the following\n",
        "\n",
        "  $Recall@K$ = $\\frac{relevant\\_items\\_in\\_the\\_first\\_k\\_positions}{total\\_relevant\\_items}$\n",
        "\n",
        "\n",
        "*   **NDCG**: the NDCG checks the relevance of recommended items and the position in which they were recommended. It values relevant recommendations that are closer to the top of the recommendation list, assigning higher scores to relevant items in higher positions. The formula for calculating the NDCG is the following\n",
        "\n",
        "  $NDCG@K$ = $\\frac{DCG@K}{IDCG@K}$\n",
        "\n",
        "  Where DCG@K (Discounted Cumulative Gain) has the following formula\n",
        "  \n",
        "  $DCG@K$ = $∑_{i=1}^{k}\\frac{2^{rel_i}-1}{log_2(i+1)}$\n",
        "\n",
        "  And IDCG@K (Ideal Discounted Cumulative Gain) is the ideal DCG@K value where all the K items recommended are relevant\n",
        "\n"
      ],
      "metadata": {
        "id": "RJJI5vrAHhlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def score(predicted, actual, metric, k=None):\n",
        "    \"\"\"\n",
        "    This function is used to compute one of the 4 possible evaluation metrics\n",
        "    thanks to which the model computed can be evaluated.\n",
        "    Precisely the four metrics that can be computed are the following:\n",
        "    - Precision: both MAP or Precision@K depending on the parameters given (if K\n",
        "    is not specified MAP is computed)\n",
        "    - Recall@K\n",
        "    - NDCG\n",
        "    \"\"\"\n",
        "    valid_metrics = ['precision', 'recall', 'ndcg']\n",
        "    if metric not in valid_metrics:\n",
        "        raise Exception(f\"Choose one valid baseline in the list: {valid_metrics}\")\n",
        "\n",
        "    if metric == 'precision':\n",
        "        if k is None:\n",
        "          m = np.mean([float(len(set(predicted[:k]) & set(actual))) / float(k)\n",
        "                     for k in range(1, len(predicted) + 1)])\n",
        "        else:\n",
        "          m = np.mean([float(len(set(predicted[:k]) & set(actual))) / float(k)])\n",
        "    elif metric == 'recall':\n",
        "        if k is None:\n",
        "            raise ValueError(\"Parameter 'k' must be specified for recall calculation.\")\n",
        "        m = np.mean([float(len(set(predicted[:k]) & set(actual))) / float(len(actual))])\n",
        "    elif metric == 'ndcg':\n",
        "        v = [1 if i in actual else 0 for i in predicted]\n",
        "        v_2 = [1 for i in actual]\n",
        "        dcg = sum([(2**i-1)/math.log(k+2, 2) for (k, i) in enumerate(v)])\n",
        "        idcg = sum([(2**i-1)/math.log(k+2, 2) for (k, i) in enumerate(v_2)])\n",
        "        m = dcg/idcg\n",
        "\n",
        "    return m\n"
      ],
      "metadata": {
        "id": "MHxyITK7Un0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ALS analysis\n",
        "In the following section an analysis about the ALS model trained in *Videogame_Recommender_System.ipynb* will be performed.\n",
        "\n",
        "Specifically the **RMSE** will be computed together with other useful metrics about the performance of the Recommender System like **MAP** (Mean Average Precision), **Average NDCG** (Average Normalized Discounted Cumulative Gain), **Precision@K** and **Recall@K**"
      ],
      "metadata": {
        "id": "IAWg8d4qS781"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-traianed model\n",
        "als_model = ALSModel.load('/content/gdrive/MyDrive/VideogameRecommenderSystem/models/ALS19_08_2023__13_09_01')"
      ],
      "metadata": {
        "id": "zihhIJx-S7EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data about the users' activity\n",
        "user_activity_df = spark.read.load('/content/gdrive/MyDrive/VideogameRecommenderSystem/user_activity',\n",
        "                           format=\"csv\",\n",
        "                           sep=\",\",\n",
        "                           inferSchema=\"true\",\n",
        "                           header=\"true\")"
      ],
      "metadata": {
        "id": "AGMwhWs3brVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect some row samples\n",
        "user_activity_df.show(n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0-jHuscb0nq",
        "outputId": "575a0359-4379-46b4-da18-27d42646cec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------+---------------+------+------------+\n",
            "|         steam_id|steam_appid|    recommended|rating|steam_int_id|\n",
            "+-----------------+-----------+---------------+------+------------+\n",
            "|76561198259233672|     951650|    Recommended|   3.0|           0|\n",
            "|76561198341423566|    1581451|    Recommended|   3.0|           1|\n",
            "|76561198031569409|       2310|    Recommended|   3.5|           2|\n",
            "|76561198031569409|     337000|    Recommended|   5.0|           2|\n",
            "|76561198031569409|    1353270|Not Recommended|   2.0|           2|\n",
            "+-----------------+-----------+---------------+------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like in the model training it is necessary to divide the dataset in training and test set, in order to have the same RMSE computed during the training the same seed, stored in RANDOM_SEED, will be used"
      ],
      "metadata": {
        "id": "s4Sh7V5YpkwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(training, test) = user_activity_df.randomSplit([0.8, 0.2], seed=RANDOM_SEED)"
      ],
      "metadata": {
        "id": "s2pJS3hVUGdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tables with the predictions computed over the unkown data of the test set will be computed and in this way the RMSE will be presented"
      ],
      "metadata": {
        "id": "JX0Q7qXSpwvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = als_model.transform(test)"
      ],
      "metadata": {
        "id": "G6xYiyklTD9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = RegressionEvaluator(\n",
        "    metricName=\"rmse\",\n",
        "    labelCol=\"rating\",\n",
        "    predictionCol=\"prediction\"\n",
        ")"
      ],
      "metadata": {
        "id": "NbDd6UDIcRVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the model do not perform too poorley, since the RMSE is 0.94 and a good value to be expected is at most 0.85"
      ],
      "metadata": {
        "id": "kozX3A9_p6wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NELJBiOqTM9Q",
        "outputId": "ff5e21f7-06a3-47fc-8d2f-e481da473403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE): 0.9436523803639781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two ways to compute recommendations with the model:\n",
        "\n",
        "1.   **recommendForUserSubset**: with this function it is needed to provide a subset of users on which compute the recommendation\n",
        "2.   **recommendForAllUsers**: with this function the recommendations are computed for all the users\n",
        "\n",
        "It is important to notice that in both cases it is necessary to explicitly say how many recommendations must be computed\n",
        "\n"
      ],
      "metadata": {
        "id": "7lToBY0UqVSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a subset with a single sample user (having as steam_int_id 168492)\n",
        "user = spark.createDataFrame([168492], \"int\").toDF(\"steam_int_id\")"
      ],
      "metadata": {
        "id": "LatA1SoPTSAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the recommendation for the user\n",
        "steam_id = 168492\n",
        "user_recs = als_model.recommendForUserSubset(user, 5)\n",
        "print(f\"Top 5 recommendations for User {steam_id}:\")\n",
        "user_recs.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3K65EiDTTzJ",
        "outputId": "9021a850-1f1a-457b-cfc2-30d2f2042115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 recommendations for User 168492:\n",
            "+------------+----------------------------------------------------------------------------------------------------------+\n",
            "|steam_int_id|recommendations                                                                                           |\n",
            "+------------+----------------------------------------------------------------------------------------------------------+\n",
            "|168492      |[{1145020, 5.032404}, {1487360, 5.020478}, {2134970, 5.0194483}, {2301630, 5.016604}, {1096060, 5.002679}]|\n",
            "+------------+----------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate top-k recommendations for each user\n",
        "userRecs = als_model.recommendForAllUsers(5)  # Top-5 recommendations for each user\n",
        "\n",
        "# Prepare the input for RankingMetrics\n",
        "user_ground_truth = test.groupby('steam_int_id').agg(sqlf.collect_list('steam_appid').alias('ground_truth_items'))\n",
        "user_train_items = training.groupby('steam_int_id').agg(sqlf.collect_list('steam_appid').alias('train_items'))\n",
        "\n",
        "# Join the recommendations and ground truth data on the user ID\n",
        "user_eval = userRecs.join(user_ground_truth, on='steam_int_id').join(user_train_items, on='steam_int_id') \\\n",
        "    .select('steam_int_id', 'recommendations.steam_appid', 'ground_truth_items', 'train_items', 'recommendations.rating')\n",
        "user_eval = user_eval.toPandas()"
      ],
      "metadata": {
        "id": "249VEkrXTgu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the lists of the items and the ratings (that corresponds in the indexes\n",
        "# meaning that in position i of the list there is an item j for which corresponds\n",
        "# a rating k that is in position i in the list of the ratings) that were not used\n",
        "# for the training of the model\n",
        "user_eval['itemIndex_filtered'] = user_eval.apply(lambda x:[b for (b,z) in zip(x.steam_appid, x.rating) if b not in x.train_items], axis=1)\n",
        "user_eval['rating_filtered'] = user_eval.apply(lambda x:[z for (b,z) in zip(x.steam_appid, x.rating) if b not in x.train_items], axis=1)"
      ],
      "metadata": {
        "id": "I-kONBedUght"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute MAP, NDCG, Recall@K and Precision@K with the previous function called\n",
        "# 'score'\n",
        "user_eval['precision'] = user_eval.apply(lambda x: score(x.itemIndex_filtered, x.ground_truth_items, 'precision'), axis=1)\n",
        "user_eval['NDCG'] = user_eval.apply(lambda x: score(x.itemIndex_filtered, x.ground_truth_items, 'ndcg'), axis=1)"
      ],
      "metadata": {
        "id": "V29zA0ZwUqkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_eval['recall'] = user_eval.apply(lambda x: score(x.itemIndex_filtered, x.ground_truth_items, 'recall', 5), axis=1)\n",
        "user_eval['precisionAt'] = user_eval.apply(lambda x: score(x.itemIndex_filtered, x.ground_truth_items, 'precision', 5), axis=1)"
      ],
      "metadata": {
        "id": "mGydWGkJZRy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAP = user_eval.precision.mean()\n",
        "avg_NDCG = user_eval.NDCG.mean()\n",
        "recall = user_eval.recall.mean()\n",
        "precision = user_eval.precisionAt.mean()"
      ],
      "metadata": {
        "id": "0vFU_nOmUtPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the values computed the model, even though it has a not-so-bad RMSE, it has some problems during the recommendation computation since the values for the metrics are the following:\n",
        "\n",
        "\n",
        "*   MAP: 0.0000029\n",
        "*   avg_NDCG: 0.0000022\n",
        "*   Recall@K (K=5): 0.0000023\n",
        "*   Precision@K (K=5): 0.0000033\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cHBeiBaNFdFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The Mean Average Precision (MAP) is: {MAP}')\n",
        "print(f'The Average Normalized Discounted Cumulative Gain (avg_NDCG) is {avg_NDCG}')\n",
        "print(f'The Recall@K with K=5 is {recall}')\n",
        "print(f'The Precision@K with K=5 is {precision}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bjkz4pYQUvql",
        "outputId": "4a0218f9-c824-468d-9765-381ffc38424b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Mean Average Precision (MAP) is: 2.9196108798761012e-05\n",
            "The Average Normalized Discounted Cumulative Gain (avg_NDCG) is 2.203468619847804e-05\n",
            "The Recall@K with K=5 is 2.3468331061438896e-05\n",
            "The Precision@K with K=5 is 3.2910657245184406e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the data for the Demo\n",
        "In this section the data sample for the demo is created; in order to make the web application as light as possible in the loadings it has been chosen to reduce the sample of players for which compute the recommendation from (more or less) 131000 to 20"
      ],
      "metadata": {
        "id": "_hycC8dtpFlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_20_distinct_values = user_activity_df.groupBy('steam_id').count().orderBy(sqlf.col(\"count\").desc()).limit(20)"
      ],
      "metadata": {
        "id": "L68034PgxXFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_df = user_activity_df.join(top_20_distinct_values, on='steam_id', how=\"inner\")"
      ],
      "metadata": {
        "id": "CnXPwFlSxoSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_df = demo_df.drop(\"count\")"
      ],
      "metadata": {
        "id": "Bxn1xs-Uxu0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_df.write.options(header='True', delimiter=',').csv('/content/gdrive/MyDrive/VideogameRecommenderSystem/demo_data')"
      ],
      "metadata": {
        "id": "e013k5NUyJ7Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}