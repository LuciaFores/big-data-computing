{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NgLK_AYNXz1E",
        "lbwaTeURbK0m"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CgHP8Kn29Qw6"
      },
      "outputs": [],
      "source": [
        "JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "RANDOM_SEED = 42 # for reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparazione"
      ],
      "metadata": {
        "id": "NgLK_AYNXz1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wpRFM3E9ZUw",
        "outputId": "55d473da-a68b-4ab2-e6d5-15c7d91e7d43"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19867 sha256=1be3368d549688a023c68e0ebefc74160833b01b86ad86c530b0060576c21f92\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/42/78/0c3d438d7f5730451a25f7ac6cbf4391759d22a67576ed7c2c\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2Tn9uykXlYRjLcSLIgmv1iLdEvG_3gqA9M3V4hLGqH9N3JWEr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PfHfzyv9hIQ",
        "outputId": "5783d8c3-5e48-417f-cfad-ceaf148dcb3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install openjdk-8-jdk-headless -qq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCKtaZTY9jJj",
        "outputId": "d2d111ee-0bf2-4d60-fcad-ad544e284354"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 39.7 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 120831 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u382-ga-1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u382-ga-1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "import re"
      ],
      "metadata": {
        "id": "r_LKWaUz9pym"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME"
      ],
      "metadata": {
        "id": "BHU1hLFz9xsf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect this colab to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-8Vm85H9yUz",
        "outputId": "45824285-e826-40fc-8d15-7fc721991bff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PySpark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFDKkWjI911V",
        "outputId": "f1d6f5b2-9dca-4efe-8d87-d14bd90558ef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285388 sha256=905c4f687c582d4ee49a107f096e72a23d7a897aca3642b03f352b7516dc0941\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "# POI CAPISCI QUALE TI SERVE VERAMENTE\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as sqlf\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from pyspark.ml.feature import HashingTF, CountVectorizer, IDF\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.feature import Normalizer\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "import gc"
      ],
      "metadata": {
        "id": "Nln3r48J96zX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the session\n",
        "# POI CAPISCI SE SONO I PARAMETRI GIUSTI\n",
        "\n",
        "\n",
        "# Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '4G').\\\n",
        "                set('spark.driver.memory', '45G').\\\n",
        "                set('spark.driver.maxResultSize', '10G').\\\n",
        "                set('setCheckpointDir', '/content/gdrive/MyDrive/VideogameRecommenderSystem/checkpoints_pyspark').\\\n",
        "                setAppName(\"VideogameRecommenderSystem\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "DYDPtkph-Akb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "port = '4050'\n",
        "public_url = ngrok.connect(port).public_url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN2cVyU0EBJ2",
        "outputId": "79843ab1-d42e-4d72-e4bf-592fe4850595"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-08-16T08:53:44+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"To access the Spark Web UI console, please click on the following link to the ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))"
      ],
      "metadata": {
        "id": "zMgHnFAQ-DRP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf4c71f-c3fe-4cea-8cad-3b5c2ec7bc26"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To access the Spark Web UI console, please click on the following link to the ngrok tunnel \"https://856d-34-74-132-107.ngrok.io\" -> \"http://127.0.0.1:4050\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "id": "PJNiUdij-GFm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "c85f0c30-2f7d-4e58-d3ef-4edf512ac16a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7bda0ab525f0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://2171d65b70b9:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>VideogameRecommenderSystem</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "games_df = spark.read.load('/content/gdrive/MyDrive/VideogameRecommenderSystem/games.csv',\n",
        "                           format=\"csv\",\n",
        "                           sep=\",\",\n",
        "                           inferSchema=\"true\",\n",
        "                           header=\"true\")"
      ],
      "metadata": {
        "id": "WV6Zb2g_-IBx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Row number:\", games_df.count(), \"\\nColumn number:\", len(games_df.columns))"
      ],
      "metadata": {
        "id": "qm0p184H-L48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8cf5997-228e-42a0-8683-86f66abb0029"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row number: 131974 \n",
            "Column number: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the links from the games descriptions since they are not relevant for\n",
        "# the project\n",
        "CLEANER_LINKS = re.compile('http[s]?://\\S+')\n",
        "\n",
        "def clean_links(raw_text):\n",
        "  cleantext = re.sub(CLEANER_LINKS, '', raw_text)\n",
        "  return cleantext"
      ],
      "metadata": {
        "id": "r0aPif0x-jCt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SPIEGA PERCHÉ HAI BISOGNO DI STO UDF\n",
        "clean_links_UDF = sqlf.udf(lambda x : clean_links(x), StringType())"
      ],
      "metadata": {
        "id": "qa8oT9lA-j5N"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLEANER_PUNCTUATION = re.compile('[^\\w\\s]')\n",
        "\n",
        "def clean_punctuation(raw_text):\n",
        "  cleantext = re.sub(CLEANER_PUNCTUATION, ' ', raw_text)\n",
        "  return cleantext"
      ],
      "metadata": {
        "id": "WtJYO5S4-5FC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_punctuation_UDF = sqlf.udf(lambda x : clean_punctuation(x), StringType())"
      ],
      "metadata": {
        "id": "KSE9LSbp-7De"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLEANER_EXTRA_SPACES = re.compile('\\s\\s+')\n",
        "\n",
        "def clean_extra_spaces(raw_text):\n",
        "  cleantext = re.sub(CLEANER_EXTRA_SPACES, ' ', raw_text)\n",
        "  return cleantext"
      ],
      "metadata": {
        "id": "YhoGlC-v_BRy"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_extra_spaces_UDF = sqlf.udf(lambda x : clean_extra_spaces(x), StringType())"
      ],
      "metadata": {
        "id": "lTE7x10z_DQI"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@sqlf.udf(\"long\")\n",
        "def num_nonzeros(v):\n",
        "  return v.numNonzeros()"
      ],
      "metadata": {
        "id": "kAPlLfQrr0HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data_for_clustering(dataset_df,\n",
        "                                column_name):\n",
        "  dataset_df = dataset_df.dropna()\n",
        "  dataset_df = dataset_df.withColumn(column_name, clean_links_UDF(sqlf.col(column_name)))\n",
        "  dataset_df = dataset_df.withColumn(column_name, sqlf.trim(sqlf.col(column_name)))\n",
        "\n",
        "  dataset_df = dataset_df.where(dataset_df.total_recommendations > 0)\n",
        "\n",
        "  dataset_df = dataset_df.withColumn(column_name, sqlf.lower(sqlf.col(column_name)))\n",
        "  dataset_df = dataset_df.withColumn(column_name, clean_punctuation_UDF(sqlf.col(column_name)))\n",
        "  dataset_df = dataset_df.withColumn(column_name, clean_extra_spaces_UDF(sqlf.col(column_name)))\n",
        "\n",
        "  tokenizer = Tokenizer(inputCol=column_name, outputCol='tokens')\n",
        "  tokens_df = tokenizer.transform(dataset_df)\n",
        "\n",
        "  stopwords_remover = StopWordsRemover(inputCol='tokens', outputCol='terms')\n",
        "  terms_df = stopwords_remover.transform(tokens_df)\n",
        "\n",
        "  stemmer = SnowballStemmer(language=\"english\")\n",
        "  stemmer_udf = sqlf.udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
        "  terms_stemmed_df = terms_df.withColumn('terms_stemmed', stemmer_udf('terms'))\n",
        "\n",
        "  cv = CountVectorizer(inputCol='terms_stemmed', outputCol=\"tf_features\", vocabSize=1000, minDF=10)\n",
        "  idf = IDF(inputCol=\"tf_features\", outputCol=\"features\")\n",
        "\n",
        "  pipeline = Pipeline(stages=[cv, idf])\n",
        "  features = pipeline.fit(terms_stemmed_df)\n",
        "  tf_idf_df = features.transform(terms_stemmed_df)\n",
        "\n",
        "  tf_idf_df = tf_idf_df.where(num_nonzeros(\"features\") > 0)\n",
        "\n",
        "  return tf_idf_df"
      ],
      "metadata": {
        "id": "P2fZHkChnjvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf_df = process_data_for_clustering(games_df, 'about_the_game')"
      ],
      "metadata": {
        "id": "UUmfqOkWqxQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tf_idf_df.count()"
      ],
      "metadata": {
        "id": "DTTDo_D_aFJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funzioni"
      ],
      "metadata": {
        "id": "lbwaTeURbK0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def k_means(dataset,\n",
        "            n_clusters,\n",
        "            features_col='features',\n",
        "            prediction_col='cluster',\n",
        "            random_seed=RANDOM_SEED):\n",
        "\n",
        "  print(\"\"\"K-means parameter:\n",
        "  - K (number of clusters) = {:d}\n",
        "  - random seed = {:d}\n",
        "  \"\"\".format(n_clusters, random_seed))\n",
        "\n",
        "  dataset = Normalizer(inputCol=features_col, outputCol=features_col+\"_norm\", p=1).transform(dataset)\n",
        "  features_col = features_col+\"_norm\"\n",
        "\n",
        "  k_means = KMeans(\n",
        "      featuresCol=features_col,\n",
        "      predictionCol=prediction_col,\n",
        "      k=n_clusters,\n",
        "      initMode='k-means||',\n",
        "      initSteps=5,\n",
        "      tol=0.000001,\n",
        "      maxIter=20,\n",
        "      seed=random_seed,\n",
        "      distanceMeasure='cosine'\n",
        "      )\n",
        "  model = k_means.fit(dataset)\n",
        "\n",
        "  clusters_df = model.transform(dataset)\n",
        "\n",
        "  return model, clusters_df"
      ],
      "metadata": {
        "id": "aapxqUY9RqQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_k_means(clusters,\n",
        "                     metric_name=\"silhouette\",\n",
        "                     distance_measure=\"cosine\",\n",
        "                     prediction_col=\"cluster\"\n",
        "                     ):\n",
        "\n",
        "  evaluator = ClusteringEvaluator(metricName=metric_name,\n",
        "                                  distanceMeasure=distance_measure,\n",
        "                                  predictionCol=prediction_col\n",
        "                                  )\n",
        "\n",
        "  return evaluator.evaluate(clusters)"
      ],
      "metadata": {
        "id": "1DwvMD6RKhFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering"
      ],
      "metadata": {
        "id": "nJd9teRoX30n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 100\n",
        "print(\"Running K-means using K = {:d}\".format(k))\n",
        "model, clusters_df = k_means(tf_idf_df, k)\n",
        "silhouette_k = evaluate_k_means(clusters_df)\n",
        "wssd_k = model.summary.trainingCost\n",
        "print(\"Silhouette coefficient computed with cosine distance: {:.3f}\".format(silhouette_k))\n",
        "print(\"Within-cluster Sum of Squared Distances (using cosine distance): {:.3f}\".format(wssd_k))\n",
        "print(\"--------------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "GgATh4WHXCXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/gdrive/MyDrive/VideogameRecommenderSystem/models/KMEANS_about_K_{}'.format(k)\n",
        "model.save(path)"
      ],
      "metadata": {
        "id": "hKm5sZddFyJI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}